# Refined Document Processing Pipeline V2

## ğŸ¯ **Final Optimized Sequence**

```
1. Ingest
   â”œâ”€ Extract 1008 data
   â””â”€ Store all PDFs in document_analysis

2. Dedup (hash-based)
   â”œâ”€ Text hash (SHA-256) for exact duplicates
   â””â”€ Visual hash (phash/dhash/ahash) for similar docs

3. Deep Extract (Opus page-wise)
   â”œâ”€ Page-by-page JSON extraction
   â”œâ”€ Document summary (metadata-rich)
   â””â”€ Stores in individual_analysis->document_summary

3.1. Deep Extract (Small pages failed)
    â””â”€ Retry extraction for docs < 20 pages that failed

4. Metadata Extraction
   â”œâ”€ Extract from document_summary:
   â”‚  â”œâ”€ document_type
   â”‚  â”œâ”€ borrower_name, co_borrower_name
   â”‚  â”œâ”€ document_date, period_covered
   â”‚  â”œâ”€ has_signature
   â”‚  â”œâ”€ issuer, account_numbers
   â”‚  â”œâ”€ financial details (income, debt, loan amounts)
   â”‚  â””â”€ completeness status
   â””â”€ Store in version_metadata

5. Global Classification
   â”œâ”€ Analyze ALL documents together with Claude
   â”œâ”€ Classification: FINANCIAL vs NON-FINANCIAL
   â”œâ”€ Document Type: Standard industry names
   â”œâ”€ Dates: YYYY-MM-DD format
   â”œâ”€ Signers: Borrower/Co-Borrower/Both/Unsigned
   â”œâ”€ Signed Status: Signed or Unsigned
   â”œâ”€ Grouping: Related docs (siblings, versions, duplicates)
   â””â”€ Primary Selection: Best/latest version in each group

6. AI Grouping
   â”œâ”€ Semantic document understanding (Claude VLM)
   â”œâ”€ Groups related documents by meaning
   â”œâ”€ Groups versions (Initial â†’ Preliminary â†’ Final)
   â”œâ”€ Creates ai_group_id
   â””â”€ Handles complex relationships visual hashing misses

7. AI Versioning
   â”œâ”€ Uses ai_group_id from Step 6
   â”œâ”€ Determines latest version intelligently
   â”œâ”€ Handles version progressions
   â”œâ”€ Updates is_latest_version flag
   â””â”€ Sets status: master/unique/superseded

8. Knowledge Graph Creation
   â”œâ”€ Filter: ONLY latest version docs (is_latest_version = TRUE)
   â”œâ”€ Build revised full JSON from deduplicated latest versions
   â”œâ”€ Batch process in 100K token chunks (~400K chars)
   â”œâ”€ Extract entities and relationships
   â”œâ”€ Build compressed knowledge graph
   â””â”€ Incremental saving after each batch
```

---

## ğŸ“Š **Key Changes from V1**

### **Removed Steps (Redundant):**
- âŒ `step4a_filename_audit.py` - Covered by Global Classification (Step 5)
- âŒ `step4b_generate_summaries.py` - Covered by Global Classification (Step 5)
- âŒ `step7_reversioning.py` (old) - Replaced by AI Versioning (Step 7)
- âŒ `step5_financial_classification.py` - Replaced by Global Classification (Step 5)

### **New Steps (AI-Powered):**
- âœ… `step6_global_classification.py` - Comprehensive classification + grouping
- âœ… `step3_comprehensive_grouping.py` - AI semantic document grouping
- âœ… `step7_apply_ai_versioning.py` - AI-driven version identification

---

## ğŸ” **What Each Step Provides**

| Step | Script | Output Fields | Purpose |
|------|--------|---------------|---------|
| 1 | `processing.py` | - | Ingest PDFs, extract 1008 |
| 2 | `dedup_task.py` | `text_hash`, `visual_phash/dhash/ahash`, `status: duplicate/unique` | Remove exact duplicates |
| 3 | `step5_deep_extraction.py` | `individual_analysis->document_summary` | Rich page-wise JSON + summary |
| 4 | `step4_extract_metadata.py` | `version_metadata->{document_type, borrower_name, document_date, has_signature, etc}` | Structured metadata from summary |
| 5 | `step6_global_classification.py` | `version_metadata->{classification, doc_type, doc_date, signers, signed_status, group_id, is_primary}` | Full classification + grouping |
| 6 | `step3_comprehensive_grouping.py` | `version_metadata->ai_group_id` | Semantic document groups |
| 7 | `step7_apply_ai_versioning.py` | `is_latest_version`, `status: master/unique/superseded` | Latest version identification |
| 8 | `step6_generate_knowledge_graph.py` | `loans->knowledge_graph` (JSONB) | Compressed entity-relationship graph (latest versions only) |

---

## ğŸ’¡ **Why This Solves the URLA Problem**

### Before (Visual Hashing Only):
```
initial_urla_42.pdf        â†’ Group 077f6d90 â†’ status: unique â†’ is_latest: TRUE
urla___preliminary_71.pdf  â†’ Group 077f6d90 â†’ status: unique â†’ is_latest: TRUE
urla___final_70.pdf        â†’ No Group      â†’ status: unique â†’ is_latest: TRUE
```
**Problem**: All 3 marked as "latest" â†’ Knowledge graph has duplicate/conflicting info

### After (AI Grouping + Versioning + Clean KG):
```
Step 6 (AI Grouping):
  â†’ Claude recognizes all 3 are URLA 1003 forms
  â†’ Creates ai_group_id: "urla_1003_group"
  â†’ Groups them together by semantic meaning

Step 7 (AI Versioning):
  â†’ Within "urla_1003_group":
    - initial_urla_42.pdf    â†’ is_latest: FALSE, status: superseded
    - urla___preliminary_71.pdf â†’ is_latest: FALSE, status: superseded
    - urla___final_70.pdf    â†’ is_latest: TRUE, status: master

Step 8 (Knowledge Graph):
  â†’ Query filters: WHERE is_latest_version = TRUE
  â†’ ONLY urla___final_70.pdf is included in KG
  â†’ Superseded versions are excluded
```
**Result**: Clean knowledge graph with no duplicate/conflicting URLA information

---

## ğŸš€ **Next Steps**

1. âœ… **Pipeline Updated** - `batch_process_loans.py` now uses refined sequence
2. â³ **Test on Loan 27** - Run refined pipeline to verify URLA versioning
3. â³ **Knowledge Graph** - Regenerate KG with clean document versions
4. â³ **Batch Process** - Apply to all 12 loans

---

## ğŸ“ **Implementation Details**

### Script Locations:
- **Batch Orchestrator**: `batch_process_loans.py`
- **Global Classification**: `step6_global_classification.py`
- **AI Grouping**: `step3_comprehensive_grouping.py`
- **AI Versioning**: `step7_apply_ai_versioning.py`
- **Knowledge Graph**: `step6_generate_knowledge_graph.py`

### Execution:
```bash
# Single loan
python3 batch_process_loans.py --loans loan_1642451 --skip-copy --concurrency 10

# All loans
python3 batch_process_loans.py --concurrency 10
```

### Expected Time:
- **Deep Extraction**: 2-4 hours (largest time sink)
- **Metadata Extraction**: 5-10 minutes
- **Global Classification**: 30-60 minutes (Claude analyzes all docs)
- **AI Grouping**: 1-2 hours (VLM for semantic understanding)
- **AI Versioning**: 15-30 minutes
- **Total**: ~4-8 hours per loan

---

## âœ¨ **Benefits**

1. **Accurate Versioning**: AI understands document relationships
2. **Clean Knowledge Graph**: No duplicate/conflicting information
3. **Reduced Redundancy**: Removed 4 redundant scripts
4. **Comprehensive Metadata**: All classification in one place
5. **Scalable**: Same process works for all document types
6. **Maintainable**: Clear, documented pipeline

